<?xml version="1.0" encoding="UTF-8"?>
<chapter id="performance-tuning">
	<title>Performance Tuning</title>

	<section id="performance-dos">
		<title>Performance "Do"s</title>

		<section id="performance-ram">
			<title>Lots of RAM</title>
			
			<para>
				OpenNMS is not terribly heavy on CPU usage, but is <emphasis>extremely</emphasis>
				I/O-bound, and will also take advantage of as much RAM as you can give it.
				OpenNMS itself doesn't use a huge amount of RAM per-node, but allowing the OS to
				cache filesystem interaction makes a very large performance difference.
			</para>
		</section>
		
		<section id="battery-backed-write-cache">
			<title>Battery-Backed Write Cache</title>

			<para>
				If you are running on hardware RAID, it is strongly recommended that you
				have a battery-backed write cache.  For example, one user reported that on
				an HP DL380 G4, the I/O wait of the server dropped from 15% to essentially
				nothing, using a 128MB battery-backed write cache.
			</para>
		</section>

		<section id="multiple-spindles">
			<title>Multiple Spindles</title>
			
			<para>
				You will get the most out of OpenNMS if you spread your I/O out into multiple
				spindles and/or separate disks/channels.
			</para>

			<section id="multiple-spindles-postgresql">
				<title>PostgreSQL</title>
				
				<para>
					PostgreSQL writes primarly to 2 classes of files and directories.
				</para>
				
				<glosslist>
					<glossentry>
						<glossterm>the database</glossterm>
						<glossdef>
							<para>
								The main PostgreSQL database is in <filename><varname>$PGDATA</varname>/base</filename>
								(<varname>$PGDATA</varname> is usually something like
								<filename>/var/lib/pgsql/data</filename>).
							</para>
						</glossdef>
					</glossentry>
					<glossentry>
						<glossterm>the journal</glossterm>
						<glossdef>
							<para>
								PostgreSQL keeps a journal of transactions, in
								<filename><varname>$PGDATA</varname>/pg_xlog</filename>.
							</para>
						</glossdef>
					</glossentry>
				</glosslist>
				
				<para>
					If you can separate the pg_xlog directory onto another spindle or mount
					point, you will increase your PostgreSQL performance considerably.  To do
					so, you should be able to just shut down PostgreSQL, move that directory,
					symlink it to the old location, and start it back up.
					<programlisting>sudo /etc/init.d/postgresql stop
sudo mv /var/lib/pgsql/data/pg_xlog /mnt/xlogspindle/pg_xlog
sudo ln -s /mnt/xlogspindle/pg_xlog /var/lib/pgsql/data/pg_xlog
sudo /etc/init.d/postgresql start</programlisting>
				</para>
			</section>
			
			<section id="multiple-spindles-rrd">
				<title>Round-Robin (Collection and Performance) Data</title>
				
				<para>
					The RRD data is the single heaviest source of I/O in most OpenNMS
					installations.  Making sure that it is on a different spindle from
					PostgreSQL makes a huge difference.
				</para>

				<itemizedlist>
					<listitem>
						<para>RRD data storage causes a large number of small random
						disk writes, usually a few writes for each update.</para>
					</listitem>
					<listitem>
						<para>By default, OpenNMS stores each collected variable in its
						own file, unless the store by group feature is enabled.</para>
					</listitem>
					<listitem>
						<para>Normally, there will be 2-3 writes for each update: one for
						the file header, one for the previous RRA, one for the next RRA.</para>
					</listitem>
					
					<listitem>
						<para>When multiple samples are consolidated into a single stored
						data point in the RRA, there will be additional writes.  By
						default, such consolidations happen hourly and daily on the GMT
						day boundary.  This will cause higher than normal amount of
						writes after the top of the hour and after the GMT day boundary.</para>
					</listitem>
				</itemizedlist>

				<para>
					The OpenNMS RRDs live, by default, in <filename><varname>$OPENNMS_HOME</varname>/share</filename>.
					If you are using the RPMs, this will be <filename>/var/opennms</filename> instead.
					<programlisting>sudo mv /var/opennms /mnt/rrdspindle/opennms
sudo rm -f /opt/opennms/share
sudo ln -s /mnt/rrdspindle/opennms /opt/opennms/share</programlisting>
				</para>
			</section>
			
		</section>

		<section id="performance-noatime">
			<title>Use <varname>noatime</varname> on OpenNMS Data Spindles on Linux and Solaris</title>
			
			<para>
				If you are dedicating spindles or drives to OpenNMS, you can mount them
				with the <varname>noatime</varname> option on Linux or Solaris for an
				additional performance boost.  This will keep the OS from updating the
				file access time on individual RRD and database files every time they
				are used.
			</para>
			
			<para>
				On Linux, you do so by editing <filename>/etc/fstab</filename> and adding
				<varname>noatime</varname> to the options section of the filesystem.  For example:
				<programlisting>LABEL=/                           /                           ext3    defaults         1 1
LABEL=/var/opennms                /var/opennms                ext3    defaults,noatime 1 2
LABEL=/var/lib/pgsql              /var/lib/pgsql              ext3    defaults,noatime 1 2
LABEL=/var/lib/pgsql/data/pg_xlog /var/lib/pgsql/data/pg_xlog ext3    defaults,noatime 1 2</programlisting>
			</para>

			<para>
				On Solaris, you edit <filename>/etc/vfstab</filename> and add
				<varname>noatime</varname> as an option at the end of the mountpoint
				information, like so:
				<programlisting>/dev/dsk/c1d0s0 /dev/rdsk/c1d0s0 /                            ufs 1 no
/dev/dsk/c1d1s0 /dev/rdsk/c1d1s0 /opt/opennms/share            ufs 2 no noatime
/dev/dsk/c1d2s0 /dev/rdsk/c1d2s0 /usr/local/pgsql/data         ufs 2 no noatime
/dev/dsk/c1d3s0 /dev/rdsk/c1d3s0 /usr/local/pgsql/data/pg_xlog ufs 2 no noatime</programlisting>
			</para>

		</section>
		
		<section id="performance-raid">
			<title>RAID Drives</title>
			
			<para>
				Use a mirrored stripe (RAID-10), with enough disks to handle the amount of data
				you need to collect.  A single disk, a pair of mirrored disks (RAID-1), or a
				RAID-5 is only appropriate for an installation doing a small amount of data
				collection.
			</para>
			
		</section>
		
		<section id="performance-postgresql">
			<title>PostgreSQL Performance Tuning</title>
			
			<para>
				There are a number of other things you can do to tune PostgreSQL.  For a good
				writeup on PostgreSQL performance tuning, see <ulink
				url="http://revsys.com/writings/postgresql-performance.html">this page at revsys.com</ulink>.
			</para>
			
			<section id="performance-postgresql-8.1">
				<title>PostgreSQL 8.1-specific Recommendations</title>
				
				<para>
					If you have a reasonable amount of RAM (2GB+), the following settings should
					give much better performance than the defaults that come with the PostgreSQL
					configuration:
					<programlisting>shared_buffers = 20000
work_mem = 16348
maintenance_work_mem = 65536
vacuum_cost_delay = 50
checkpoint_segments = 20
checkpoint_timeout = 900
wal_buffers = 64
stats_start_collector = on
stats_row_level = on
autovacuum = on</programlisting>
				</para>
			</section>
			
			<section id="performance-postgresql-8.2">
				<title>PostgreSQL 8.2+ Recommendations</title>
				
				<para>
					On systems with 4GB or more of RAM, we've found that changing the
					max_fsm_pages and max_fsm_releations, as well as work_mem and
					maintenance_work_mem improves performance dramatically:
					<programlisting>work_mem = 100MB
maintenance_work_mem = 128MB

#max_fsm_pages = 204800		# min max_fsm_relations*16, 6 bytes each
max_fsm_pages = 2048000
#max_fsm_relations = 1000		# min 100, ~70 bytes each
max_fsm_relations = 10000</programlisting>
				</para>
			</section>

			<note>
				<para>
					If you increase memory settings for PostgreSQL, you will probably need
					to increase the maximum shared-memory settings in your OS.  On Linux,
					you can do this by editing <filename>/etc/sysctl</filename> and adding
					the line:
					<code>kernel.shmmax=170639360</code>
				</para>
				<para>
					Depending on how many shared memory segments you need, you may need to
					adjust that value.
				</para>
			</note>
	
		</section>
	</section>
	
	<section id="performance-donts">
		<title>Performance "Don't"s</title>

		<para>
			Because of OpenNMS's high-I/O profile, there are a number of things that will
			cause performance issues on reasonably large installs.
		</para>
		
		<itemizedlist>
			<listitem>
				<para>Don't run in a VM (although some pseudo-VMs like <ulink
				url="http://www.xen.org/">Xen</ulink> are not as hard on I/O as things like
				<ulink url="http://www.vmware.com/">VMware</ulink>).
				</para>
			</listitem>
			<listitem>
				<para>Don't put the database or RRD data on file systems managed by LVM.</para>
			</listitem>
			<listitem>
				<para>Don't put DB or RRD data on file systems on RAID-5.</para>
			</listitem>
			<listitem>
				<para>Don't use older kernels.  Linux 2.6 and Solaris 10 perform much
				better than older releases.</para>
			</listitem>
		</itemizedlist>
	</section>

</chapter>